{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Exploring AJILE12 dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/neurovium/Neuromatch-AJILE12/blob/master/Notebook/exploreAJILE12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can get cumbersome to manually dissect an NWB file with print statements. There are a few ways to view an NWB graphically instead. A great way to do this in a Jupyter notebook is with **[NWBWidgets](https://github.com/NeurodataWithoutBorders/nwbwidgets)**. Here, you can use NWBWidgets to view a file from a location on your machine. If you don't want to download a file just to view it, you can still use NWBWidgets to view it remotely. Check out [Streaming an NWB File with fsspec](./stream_nwb.ipynb) to learn how to do this. Another way to explore an NWB file, that doesn't require Jupyter, is with [HDFView](https://www.hdfgroup.org/downloads/hdfview/)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is meant to be run in a Google Colab notebook. It checks if the notebook is running on a Google Colab GPU, and if it is, it clones the Neuromatch-AJILE12 repository from GitHub, changes the current working directory to the Neuromatch-AJILE12k directory, and installs the package in editable mode using pip. This is done to set up the environment for using the Neuromatch-AJILE12 package in a Google Colab notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if running on Google Colab, run this cell once, then restart the runtime and run the rest of the notebook\n",
    "import os\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    # !git clone https://github.com/neurovium/Neuromatch-AJILE12\n",
    "    !git clone https://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@github.com/neurovium/Neuromatch-AJILE12\n",
    "    %cd Neuromatch-AJILE12\n",
    "    %pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DANDI repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and plotting packages\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import natsort\n",
    "from scipy.signal import sosfiltfilt, butter, hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed for this notebook to interact with the DANDI API\n",
    "from pynwb import NWBHDF5IO\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "# Libraries needed for this notebook to interact with NWB events\n",
    "from ndx_events import LabeledEvents, AnnotatedEventsTable, Events\n",
    "\n",
    "# FSSpec is a library that allows us to read files from the cloud\n",
    "import fsspec\n",
    "# import pynwb\n",
    "\n",
    "# NWB is based on HF5, so we need this library to read NWB files\n",
    "import h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subject and session number for loading data\n",
    "sbj, session = 1, 3\n",
    "behavior_type = 'Eat'\n",
    "neural_freq_range = [80, 100]  # Hz\n",
    "ecog_ch_num = 7\n",
    "keypoint_of_interest = 'R_Wrist'\n",
    "pose_direction = 'vertical'  # 'vertical' or 'horizontal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "# This is especially useful for reading small pieces of data from a large NWB file stored remotely. First, you will need to get the location of the file.\n",
    "# Now you can get the url of a particular NWB file using the dandiset ID and the path of that file within the dandiset.\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(\"000055\").get_asset_by_path(\n",
    "        \"sub-{0:>02d}/sub-{0:>02d}_ses-{1:.0f}_behavior+ecephys.nwb\".format(sbj, session)\n",
    "    )\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_path is the url of the file on the DANDI Archive. You can now use this url to read the file using pynwb.\n",
    "s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "\n",
    "fs = CachingFileSystem(\n",
    "    fs=fsspec.filesystem(\"http\"),\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "f = fs.open(s3_path, \"rb\")\n",
    "file = h5py.File(f)\n",
    "io = NWBHDF5IO(file=file, mode='r', load_namespaces=True)\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now access the data in the file as you would normally do with NWB files.\n",
    "nwbfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "nwbfile.electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "nwbfile.electrodes[ecog_ch_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the cloud path to a variable\n",
    "from hdmf.common.table import DynamicTable\n",
    "\n",
    "# assuming you have already loaded your NWB file into memory\n",
    "electrodes = nwbfile.electrodes\n",
    "print(electrodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data about different subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to each subject's session behavior/ecephys files\n",
    "with DandiAPIClient() as client:\n",
    "    paths = []\n",
    "    for file in client.get_dandiset(\"000055\", \"draft\").get_assets_with_path_prefix(\"\"):\n",
    "        paths.append(file.path)\n",
    "paths = natsort.natsorted(paths)\n",
    "# print(paths)\n",
    "paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data characteristics including the number of good and total ECoG electrodes, \n",
    "# # hemisphere implanted, and number of recording days for each participant.\n",
    "from plot_utils import (\n",
    "    load_data_characteristics,\n",
    "    plot_ecog_descript,\n",
    ")\n",
    "\n",
    "dat_chact = load_data_characteristics()\n",
    "n_elecs_good, n_elecs_tot = dat_chact[-2], dat_chact[-1]\n",
    "part_ids = dat_chact[-3]\n",
    "\n",
    "fig = plot_ecog_descript(n_elecs_tot, n_elecs_good, part_ids, nrows=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create a dataframe with the data characteristics for each participant\n",
    "# # hemisphere implanted, and number of recording days for each participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import load_data_characteritics\n",
    "\n",
    "rec_days, hemi, surf_tot, surf_good, depth_tot, depth_good, _, part, _, _ = load_data_characteristics()\n",
    "\n",
    "ages = [\n",
    "    44, 20, 33, 19, 31, 37, 26, 33, 20, 34, 34, 22\n",
    "]  # not found in data files\n",
    "gender = [\n",
    "    'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M'\n",
    "]  # not found in data files\n",
    "surf_elecs = [str(val_good)+' / '+str(val_tot) for val_good, val_tot in zip(surf_good, surf_tot)]\n",
    "depth_elecs = [str(val_good)+' / '+str(val_tot) for val_good, val_tot in zip(depth_good, depth_tot)]\n",
    "pd.DataFrame(\n",
    "    [part, gender, ages, rec_days, hemi, surf_elecs, depth_elecs],\n",
    "    index=[\n",
    "        'Participant',\n",
    "        'Gender',\n",
    "        'Age (years)',\n",
    "        'Recording days used',\n",
    "        'Hemisphere implanted',\n",
    "        'Surface electrodes: # good / total',\n",
    "        'Depth electrodes: # good / total'\n",
    "    ]\n",
    ").T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the duration for the behaviors (Sleep/rest, Inactive, Talk, TV, Computer/phone, Eat, Other activity) in each subject\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "from plot_utils import clabel_table_create\n",
    "blocklist_labels = False  # show blocklist (True) or activity (False) label durations\n",
    "\n",
    "if blocklist_labels:\n",
    "    common_acts = [\n",
    "        'Blocklist (Data break)',\n",
    "        'Blocklist (Camera move/zoom)',\n",
    "        'Blocklist (Camera occluded)',\n",
    "        'Blocklist (Experiment)',\n",
    "        'Blocklist (Private time)',\n",
    "        'Blocklist (Tether/bandage)',\n",
    "        'Blocklist (Hands under blanket)',\n",
    "        'Blocklist (Clinical procedure)',\n",
    "    ]\n",
    "else:\n",
    "    common_acts = [\n",
    "        'Sleep/rest',\n",
    "        'Inactive',\n",
    "        'Talk',\n",
    "        'TV',\n",
    "        'Computer/phone',\n",
    "        'Eat',\n",
    "        'Other activity',\n",
    "    ]\n",
    "\n",
    "clabel_table_create(common_acts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse behavioral labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coarse labels from NWB file\n",
    "min_len = 100  # (sec) only keep times when the given label appears for longer than this amount of time at once\n",
    "\n",
    "coarse_labels = nwbfile.intervals['epochs'].to_dataframe()\n",
    "coarse_labels = coarse_labels[coarse_labels['labels'].str.contains(behavior_type)]\n",
    "coarse_labels['diff'] = coarse_labels['stop_time'] - coarse_labels['start_time']\n",
    "coarse_labels = coarse_labels[coarse_labels['diff'] > min_len]\n",
    "coarse_labels.reset_index(inplace=True, drop=True)\n",
    "\n",
    "coarse_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "coarse behavior labelling trace for one recording day. Note that the figure from the data paper combined the targeted (targeted=True) and untargeted (both first_val=True and first_val=False) behavior labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the function to plot the coarse labels\n",
    "from plot_utils import prune_clabels, plot_clabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set parameters for plotting coarse labels\n",
    "targ_tlims = [13, 17]  # targeted window to plot (in hours)\n",
    "targeted = False  # plot targeted window (True) or whole day (False)\n",
    "targ_label = 'Computer/phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the coarse labels for the targeted window\n",
    "with DandiAPIClient() as client:\n",
    "    asset = clieconnt.get_dandiset(\"000055\", \"draft\").get_asset_by_path(\n",
    "        \"sub-01/sub-01_ses-4_behavior+ecephys.nwb\"\n",
    "    )\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "\n",
    "with NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3') as io:\n",
    "    nwb = io.read()\n",
    "    clabels_orig = nwb.intervals['epochs'].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select coarse labels based on user parameters\n",
    "label_col_d = {\n",
    "    'Other activity': 0,\n",
    "    'Computer/phone': 1,\n",
    "    'Eat': 2,\n",
    "    'TV': 3,\n",
    "    'Talk': 4\n",
    "}\n",
    "\n",
    "clabels, uni_labs = prune_clabels(clabels_orig, targeted,\n",
    "                                  targ_tlims, None,\n",
    "                                  targ_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coarse labels over time\n",
    "fig = plot_clabels(clabels, uni_labs, targeted, None, targ_tlims, targlab_colind=label_col_d[targ_label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if pickle file exists\n",
    "!ls data/\n",
    "# you should get a lost of files in the data folder:\n",
    "# P01_Postcentral.npy   P05_Postcentral.npy   P09_Postcentral.npy\n",
    "# P01_Precentral.npy    P05_Precentral.npy    P09_Precentral.npy\n",
    "# P01_Temporal_Inf.npy  P05_Temporal_Inf.npy  P09_Temporal_Inf.npy\n",
    "# P01_Temporal_Mid.npy  P05_Temporal_Mid.npy  P09_Temporal_Mid.npy\n",
    "# P02_Postcentral.npy   P06_Postcentral.npy   P10_Postcentral.npy\n",
    "# P02_Precentral.npy    P06_Precentral.npy    P10_Precentral.npy\n",
    "# P02_Temporal_Inf.npy  P06_Temporal_Inf.npy  P10_Temporal_Inf.npy\n",
    "# P02_Temporal_Mid.npy  P06_Temporal_Mid.npy  P10_Temporal_Mid.npy\n",
    "# P03_Postcentral.npy   P07_Postcentral.npy   P11_Postcentral.npy\n",
    "# P03_Precentral.npy    P07_Precentral.npy    P11_Precentral.npy\n",
    "# P03_Temporal_Inf.npy  P07_Temporal_Inf.npy  P11_Temporal_Inf.npy\n",
    "# P03_Temporal_Mid.npy  P07_Temporal_Mid.npy  P11_Temporal_Mid.npy\n",
    "# P04_Postcentral.npy   P08_Postcentral.npy   P12_Postcentral.npy\n",
    "# P04_Precentral.npy    P08_Precentral.npy    P12_Precentral.npy\n",
    "# P04_Temporal_Inf.npy  P08_Temporal_Inf.npy  P12_Temporal_Inf.npy\n",
    "# P04_Temporal_Mid.npy  P08_Temporal_Mid.npy  P12_Temporal_Mid.npy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# If the pickle file does not exist, then run the following cell to create it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'https://github.com/neurovium/Neuromatch-AJILE12/tree/master/data'\n",
    "# html = requests.get(url).content\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# files = [a['href'] for a in soup.select('a.js-navigation-open') if a['href'].endswith('.npy')]\n",
    "\n",
    "# !mkdir -p data\n",
    "# for file in files:\n",
    "#     filename = file.split('/')[-1]\n",
    "#     raw_url = f'https://raw.githubusercontent.com{file.replace(\"/blob\", \"\")}'\n",
    "#     !wget -O data/{filename} {raw_url}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This should create a plot of ECoG power for a single participant, electrode, and frequency band for different grid locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from plot_utils import plot_ecog_pow\n",
    "\n",
    "# Define variables\n",
    "rois_plt = [\n",
    "    'Precentral',\n",
    "    'Postcentral',\n",
    "    'Temporal_Mid',\n",
    "    'Temporal_Inf'\n",
    "]\n",
    "sbplt_titles = [\n",
    "    'Precentral\\nGyrus',\n",
    "    'Postcentral\\nGyrus',\n",
    "    'Middle Temporal\\nGyrus',\n",
    "    'Inferior Temporal\\nGyrus'\n",
    "]\n",
    "freq_range = [3, 125]\n",
    "lp = 'data/'\n",
    "\n",
    "# Plot power spectra\n",
    "plot_ecog_pow(\n",
    "    lp,\n",
    "    rois_plt,\n",
    "    freq_range,\n",
    "    sbplt_titles,\n",
    "    part_id='P01',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_order = 4  # order of butterworth filter used to bandpass filter the ECoG data\n",
    "\n",
    "neural_data = nwbfile.acquisition['ElectricalSeries'].data\n",
    "sampling_rate = nwbfile.acquisition['ElectricalSeries'].rate  # (Hz) ECoG sampling rate\n",
    "neural_power = []\n",
    "for i in range(coarse_labels.shape[0]):\n",
    "    # Identify the start/end indices for each continuous chunk of the given behavioral label\n",
    "    start_t = int(coarse_labels.loc[i, 'start_time']*sampling_rate)\n",
    "    end_t = int(coarse_labels.loc[i, 'stop_time']*sampling_rate)\n",
    "\n",
    "    # Load data snippet\n",
    "    neur_data_curr = neural_data[start_t:end_t, ecog_ch_num]\n",
    "\n",
    "    # Bandpass filter\n",
    "    sos = butter(filter_order, neural_freq_range, btype='bandpass', output='sos', fs=sampling_rate)\n",
    "    neur_data_filtered = sosfiltfilt(sos, neur_data_curr)\n",
    "\n",
    "    # Apply Hilbert transform and convert to decibels\n",
    "    neur_pow = np.abs(hilbert(neur_data_filtered))\n",
    "    neur_pow = 10*np.log(neur_pow)\n",
    "\n",
    "    # Take the difference between neighboring timepoints\n",
    "    neural_power.append(np.diff(neur_pow))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = list(nwbfile.processing['behavior'].data_interfaces['Position'].spatial_series.keys())\n",
    "assert keypoint_of_interest in keypoints\n",
    "assert pose_direction in ['vertical', 'horizontal']\n",
    "keypoint_series = nwbfile.processing['behavior'].data_interfaces['Position'].spatial_series[keypoint_of_interest]\n",
    "sampling_rate_keypoint = keypoint_series.rate  # Hz\n",
    "keypoint_velocity = []\n",
    "for i in range(coarse_labels.shape[0]):\n",
    "    start_t = int(coarse_labels.loc[i, 'start_time']*sampling_rate_keypoint)\n",
    "    end_t = int(coarse_labels.loc[i, 'stop_time']*sampling_rate_keypoint)\n",
    "\n",
    "    # Load pose data snippet\n",
    "    pose_data_curr = keypoint_series.data[start_t:end_t, :]\n",
    "    pose_mag_curr = pose_data_curr[:, 1 if pose_direction == 'vertical' else 0]\n",
    "\n",
    "    # Convert to velocity (delta X / delta t)\n",
    "    velocity_curr = np.diff(pose_mag_curr)/(1/sampling_rate_keypoint)\n",
    "    keypoint_velocity.append(velocity_curr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(neural_power) == len(keypoint_velocity)\n",
    "measures_all = []\n",
    "for i in range(len(neural_power)):\n",
    "    # Neural power for the given chunk\n",
    "    neur_curr = neural_power[i]\n",
    "    l_neur = len(neur_curr)\n",
    "\n",
    "    # Pose velocity for the given chunk\n",
    "    accel_curr = keypoint_velocity[i]\n",
    "    l_accel = len(accel_curr)\n",
    "\n",
    "    # Downsample neural data to match pose data\n",
    "    inds_split = np.array_split(np.arange(l_neur), l_accel)\n",
    "    for j, inds in enumerate(inds_split):\n",
    "        measures_all.append([neur_curr[inds].mean(), accel_curr[j]])\n",
    "\n",
    "# Combine neural/pose data into a dataframe\n",
    "df_measures_all = pd.DataFrame(np.asarray(measures_all), columns=['Neural power (dB)', 'Keypoint velocity (pixels/sec)'])\n",
    "\n",
    "# Remove any NaN's\n",
    "df_measures_all.dropna(inplace=True)\n",
    "\n",
    "# Remove instances with velocity close to 0\n",
    "df_measures_all = df_measures_all[(df_measures_all['Keypoint velocity (pixels/sec)'] > 100) |\\\n",
    "                                  (df_measures_all['Keypoint velocity (pixels/sec)'] < -100)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures_all.corr(method='pearson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nwbwdigets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nwbwidgets package to visualize the NWB file\n",
    "from nwbwidgets import nwb2widget\n",
    "nwb2widget(nwbfile)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
