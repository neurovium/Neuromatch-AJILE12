{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring AJILE12 dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/neurovium/Neuromatch-AJILE12/blob/master/Notebook/exploreAJILE12.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scientific background \n",
    "\n",
    "The AJILE12 dataset is the largest publicly available human neurobehavioral dataset, recorded during passive clinical epilepsy monitoring. It includes synchronized intracranial neural recordings and upper body pose trajectories across 55 semi-continuous days of naturalistic movements, along with relevant metadata. The dataset was created to understand the neural basis of human movement in naturalistic scenarios and expand neuroscience research beyond constrained laboratory paradigms. It is available on The DANDI Archive in the Neurodata Without Borders (NWB) data standard and can be explored using a browser-based dashboard.\n",
    "\n",
    "For scientific background, see the following papers from [Bing Brunton lab](https://www.bingbrunton.com/bing) who graciously has released the **AJILE12** data on [DANDI](https://dandiarchive.org/dandiset/000055?search=ajile12&pos=1):\n",
    "\n",
    "**Behavioral and Neural Variability of Naturalistic Arm Movements**. eNeuro, 2021. https://doi.org/10.1523/ENEURO.0007-21.2021\n",
    "\n",
    "**Mining naturalistic human behaviors in long-term video and neural recordings**. Journal of Neuroscience Methods, 2021. https://doi.org/10.1016/j.jneumeth.2021.109199\n",
    "\n",
    "## Data\n",
    "\n",
    "Annotated Joints in Long-term Electrocorticography (AJILE12) from human participants; the dataset was recorded opportunistically during passive clinical epilepsy monitoring. AJILE12 includes synchronized intracranial neural recordings and upper body pose trajectories across 55 semi-continuous days of naturalistic movements, along with relevant metadata, including thousands of wrist movement events and annotated behavioral states. Neural recordings are available at 500 Hz from at least 64 electrodes per participant, for a total of 1280 hours. Pose trajectories at 9 upper-body keypoints, including wrist, elbow, and shoulder joints, were sampled at 30 frames per second and estimated from 118 million video frames.\n",
    "\n",
    "The following paper provides details on the data and acquisition details:\n",
    "\n",
    "**AJILE12: Long-term naturalistic human intracranial neural recordings and pose**. Scientific Data, 2022. https://doi.org/10.1038/s41597-022-01280-y \n",
    "\n",
    "<img src=\"../AJILE.png\" width=\"1200\">\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code is meant to be run in a notebook. It checks if the notebook is running on a Google Colab GPU, and if it is, it clones the Neuromatch-AJILE12 repository from GitHub, changes the current working directory to the Neuromatch-AJILE12k directory, and installs the package in editable mode using pip. This is done to set up the environment for using the Neuromatch-AJILE12 package in a Google Colab notebook. If you wish to run this code on a local machine, you can comment the next cell and simply clone the git link below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean install of AJILE12 on Google Colab; this is to prevent overwriting conflicts. You can run this once for each new run instance on colab.\n",
    "%cd /\n",
    "%rm -rf /content/Neuromatch-AJILE12/ # removes old version of AJILE12 if it exists\n",
    "%cd /content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### if running on Google Colab, run this cell once, then restart the runtime and run the rest of the notebook\n",
    "import os\n",
    "if \"COLAB_GPU\" in os.environ:\n",
    "    # !git clone https://github.com/neurovium/Neuromatch-AJILE12\n",
    "    !git clone https://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@github.com/neurovium/Neuromatch-AJILE12\n",
    "    %cd Neuromatch-AJILE12\n",
    "    %pip install -e ."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read/Download data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to data on DANDI \n",
    "The data is hosted on **[DANDI](https://dandiarchive.org/)**, BRAIN Initiative's archive for publishing and sharing neurophysiology data including electrophysiology, optophysiology, behavioral time-series, and images from immunostaining experiments. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWB format\n",
    "\n",
    "**AJILE12** is in **[NWB](https://www.nwb.org/)** format. NWB is a Hierarchical Data Format (HDF) intended for scientific data. HDF is a platform-independent file format that can be used on many different computers, regardless of the operating system that machine is running. To know more about HDF, you can visit [HDFGroup](https://portal.hdfgroup.org/display/support/Documentation). \n",
    "\n",
    "For more information about NWB, see the following papers.\n",
    "\n",
    "**Neurodata Without Borders: Creating a Common Data Format for Neurophysiology**. Neuron, 2015. https://doi.org/10.1016/j.neuron.2015.10.025\n",
    "\n",
    "**The Neurodata Without Borders ecosystem for neurophysiological data science**. eLife, 2022. https://doi.org/10.7554/eLife.78362\n",
    "\n",
    "[NWB documentation](https://nwb-overview.readthedocs.io/en/latest/) provides further information on the data structure and Python/Matlab APIs to access it.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup for read/download"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see a list of required packages, see the document [ðŸ“œ requirements.txt](../requirements.txt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical and plotting packages\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "import natsort\n",
    "from scipy.signal import sosfiltfilt, butter, hilbert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries needed for this notebook to interact with the DANDI API\n",
    "from pynwb import NWBHDF5IO\n",
    "from dandi.dandiapi import DandiAPIClient\n",
    "\n",
    "# Libraries needed for this notebook to interact with NWB events\n",
    "from ndx_events import LabeledEvents, AnnotatedEventsTable, Events\n",
    "\n",
    "# FSSpec is a library that allows us to read files from the cloud\n",
    "import fsspec\n",
    "\n",
    "# NWB is based on HF5, so we need this library to read NWB files\n",
    "import h5py"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to data on cloud\n",
    "The data is hosted on AMAZON AWS in S3 buckets. The following steps guide you to locate the data based on the **dandiset** information, setup streaming and reading the data from the cloud. Alternatively, you can access the data on **[DANDI](https://dandiarchive.org/dandiset/000055?search=ajile12&pos=1)**. If you choose to directly download from DANDI, you will need a github account. The following code will be sufficient to programatically download/stream data (either for colab notebook or for your own personal machine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subject and session number for loading dataThese parameters can be adjusted to analyze other electrodes, frequency bands, behavior types, participants, sessions, etc.\n",
    "# Example: \n",
    "# Select data from participant 1, session 3 only during times that the participant was eating. \n",
    "# ECoG data will be converted to spectral power in the gamma band (80-100 Hz) for electrode 7, which is located over the motor cortex. \n",
    "# We will also look at the vertical velocity of the right wrist.\n",
    "\n",
    "\n",
    "sbj, session = 1, 3  # participant 1, session 3\n",
    "behavior_type = 'Eat' # only analyze data during eating\n",
    "neural_freq_range = [80, 100]  # Frequency band of interest in Hz\n",
    "ecog_ch_num = 7 # electrode number over motor cortex\n",
    "keypoint_of_interest = 'R_Wrist' # right wrist movement\n",
    "pose_direction = 'vertical'  # 'vertical' or 'horizontal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "# This is especially useful for reading small pieces of data from a large NWB file stored remotely. First, you will need to get the location of the file.\n",
    "# Now you can get the url of a particular NWB file using the dandiset ID and the path of that file within the dandiset.\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(\"000055\").get_asset_by_path(\n",
    "        \"sub-{0:>02d}/sub-{0:>02d}_ses-{1:.0f}_behavior+ecephys.nwb\".format(sbj, session)\n",
    "    )\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_path is the url of the file on the DANDI Archive. You can now use this url to read the file using pynwb.\n",
    "s3_path"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note on streaming \n",
    "There are two methods for streaming NWB data from the cloud using **[PyNWB streaming](https://pynwb.readthedocs.io/en/stable/tutorials/advanced_io/streaming.html)**. Currently, Colab is natively not compatible with ROS3 (read only S3); though you can use that ROS3 method for a local machine/server. If you wish to use ROS3 (instead of fsspec) on Colab, see the details in the [ðŸ“œ \"plot util readme\"](../plot_utils/readme.md) file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# You can also read specific sections within individual data files directly from remote stores such as the DANDI Archive.\n",
    "from fsspec.implementations.cached import CachingFileSystem\n",
    "\n",
    "# Note, caching is set once per access. If you want to change the cache location, you will need to restart the kernel. \n",
    "fs = CachingFileSystem(\n",
    "    fs=fsspec.filesystem(\"http\"),\n",
    "    cache_storage=\"nwb-cache\",  # Local folder for the cache\n",
    ")\n",
    "\n",
    "f = fs.open(s3_path, \"rb\")\n",
    "file = h5py.File(f)\n",
    "io = NWBHDF5IO(file=file, mode='r', load_namespaces=True)\n",
    "nwbfile = io.read()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine NWB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check NWB file and its content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can now access the data in the file as you would normally do with NWB files.\n",
    "nwbfile"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get information about the electrodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information about the electrodes is stored in the nwbfile.electrodes table.\n",
    "nwbfile.electrodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specific information about the electrode of interest can be accessed using the electrode number.\n",
    "nwbfile.electrodes[ecog_ch_num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the cloud path to a variable\n",
    "from hdmf.common.table import DynamicTable\n",
    "\n",
    "# assuming you have already loaded your NWB file into memory\n",
    "electrodes = nwbfile.electrodes\n",
    "print(electrodes)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NWB-WIDGETS "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can get cumbersome to manually dissect an NWB file with print statements. There are a few ways to view an NWB graphically instead. A great way to do this in a Jupyter notebook is with **[NWBWidgets](https://github.com/NeurodataWithoutBorders/nwbwidgets)**. Here, you can use NWBWidgets to view a file from a location on your machine. If you don't want to download a file just to view it, you can still use NWBWidgets to view it remotely. Check out [Streaming an NWB File with fsspec](./stream_nwb.ipynb) to learn how to do this. Another way to explore an NWB file, that doesn't require Jupyter, is with [HDFView](https://www.hdfgroup.org/downloads/hdfview/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the nwbwidgets package to visualize the NWB file, explore the data, and access the metadata.\n",
    "from nwbwidgets import nwb2widget\n",
    "nwb2widget(nwbfile)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information and metadata"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each subject has multiple experimental sessions. Access that info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the path to each subject's session behavior/ecephys files\n",
    "with DandiAPIClient() as client:\n",
    "    paths = []\n",
    "    for file in client.get_dandiset(\"000055\", \"draft\").get_assets_with_path_prefix(\"\"):\n",
    "        paths.append(file.path)\n",
    "paths = natsort.natsorted(paths)\n",
    "# print(paths)\n",
    "paths"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data characteristics for each participant\n",
    "Get the list of hemisphere implanted, and number of recording days for each participant and turn it to a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plot_utils import load_data_characteristics\n",
    "\n",
    "rec_days, hemi, surf_tot, surf_good, depth_tot, depth_good, _, part, _, _ = load_data_characteristics(fs=fs)\n",
    "\n",
    "ages = [\n",
    "    44, 20, 33, 19, 31, 37, 26, 33, 20, 34, 34, 22\n",
    "]  # not found in data files\n",
    "gender = [\n",
    "    'M', 'M', 'M', 'F', 'F', 'M', 'M', 'F', 'M', 'M', 'F', 'M'\n",
    "]  # not found in data files\n",
    "surf_elecs = [str(val_good)+' / '+str(val_tot) for val_good, val_tot in zip(surf_good, surf_tot)]\n",
    "depth_elecs = [str(val_good)+' / '+str(val_tot) for val_good, val_tot in zip(depth_good, depth_tot)]\n",
    "\n",
    "# Generate a dataframe with the data characteristics\n",
    "pd.DataFrame(\n",
    "    [part, gender, ages, rec_days, hemi, surf_elecs, depth_elecs],\n",
    "    index=[\n",
    "        'Participant',\n",
    "        'Gender',\n",
    "        'Age (years)',\n",
    "        'Recording days used',\n",
    "        'Hemisphere implanted',\n",
    "        'Surface electrodes: # good / total',\n",
    "        'Depth electrodes: # good / total'\n",
    "    ]\n",
    ").T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the duration for coarse behaviors (Sleep/rest, Inactive, Talk, TV, Computer/phone, Eat, Other activity) in each subject\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count activity and blocklist coarse label durations for each participant\n",
    "from plot_utils import clabel_table_create\n",
    "blocklist_labels = False  # show blocklist (True) or activity (False) label durations\n",
    "\n",
    "if blocklist_labels:\n",
    "    common_acts = [\n",
    "        'Blocklist (Data break)',\n",
    "        'Blocklist (Camera move/zoom)',\n",
    "        'Blocklist (Camera occluded)',\n",
    "        'Blocklist (Experiment)',\n",
    "        'Blocklist (Private time)',\n",
    "        'Blocklist (Tether/bandage)',\n",
    "        'Blocklist (Hands under blanket)',\n",
    "        'Blocklist (Clinical procedure)',\n",
    "    ]\n",
    "else:\n",
    "    common_acts = [\n",
    "        'Sleep/rest',\n",
    "        'Inactive',\n",
    "        'Talk',\n",
    "        'TV',\n",
    "        'Computer/phone',\n",
    "        'Eat',\n",
    "        'Other activity',\n",
    "    ]\n",
    "    \n",
    "# Generate table\n",
    "clabel_table_create(common_acts,fs=fs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coarse behavioral labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coarse behavior labelling trace for one recording day.\n",
    "Note that the figure from the data paper combined the targeted (targeted=True) and untargeted (both first_val=True and first_val=False) behavior labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the function to plot the coarse labels\n",
    "from plot_utils import prune_clabels, plot_clabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  set parameters for plotting coarse labels\n",
    "targ_tlims = [13, 17]  # targeted window to plot (in hours)\n",
    "targeted = False  # plot targeted window (True) or whole day (False)\n",
    "targ_label = 'Computer/phone'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data and coarse labels for the targeted window\n",
    "with DandiAPIClient() as client:\n",
    "    asset = client.get_dandiset(\"000055\", \"draft\").get_asset_by_path(\n",
    "        \"sub-01/sub-01_ses-4_behavior+ecephys.nwb\"\n",
    "    )\n",
    "    s3_path = asset.get_content_url(follow_redirects=1, strip_query=True)\n",
    "f = fs.open(s3_path, \"rb\")\n",
    "file = h5py.File(f)\n",
    "with NWBHDF5IO(file=file, mode='r', load_namespaces=True) as io: \n",
    "# with NWBHDF5IO(s3_path, mode='r', load_namespaces=True, driver='ros3') as io:  #if you want to use ROS3 to stream data, use this line instead\n",
    "    nwb = io.read()\n",
    "    clabels_orig = nwb.intervals['epochs'].to_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select coarse labels based on user parameters\n",
    "label_col_d = {\n",
    "    'Other activity': 0,\n",
    "    'Computer/phone': 1,\n",
    "    'Eat': 2,\n",
    "    'TV': 3,\n",
    "    'Talk': 4\n",
    "}\n",
    "\n",
    "clabels, uni_labs = prune_clabels(clabels_orig, targeted,\n",
    "                                  targ_tlims, None,\n",
    "                                  targ_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot coarse labels over time\n",
    "fig = plot_clabels(clabels, uni_labs, targeted, None, targ_tlims, targlab_colind=label_col_d[targ_label])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral power in select frequency band for different grid locations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the information on grids/subjects\n",
    "If the pickle file does not exist, then run the following cell to create it (You may need to do this if you are running the notebook locally)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if pickle file exists\n",
    "!ls data/\n",
    "# you should get a lost of files in the data folder:\n",
    "# P01_Postcentral.npy   P05_Postcentral.npy   P09_Postcentral.npy\n",
    "# P01_Precentral.npy    P05_Precentral.npy    P09_Precentral.npy\n",
    "# P01_Temporal_Inf.npy  P05_Temporal_Inf.npy  P09_Temporal_Inf.npy\n",
    "# P01_Temporal_Mid.npy  P05_Temporal_Mid.npy  P09_Temporal_Mid.npy\n",
    "# P02_Postcentral.npy   P06_Postcentral.npy   P10_Postcentral.npy\n",
    "# P02_Precentral.npy    P06_Precentral.npy    P10_Precentral.npy\n",
    "# P02_Temporal_Inf.npy  P06_Temporal_Inf.npy  P10_Temporal_Inf.npy\n",
    "# P02_Temporal_Mid.npy  P06_Temporal_Mid.npy  P10_Temporal_Mid.npy\n",
    "# P03_Postcentral.npy   P07_Postcentral.npy   P11_Postcentral.npy\n",
    "# P03_Precentral.npy    P07_Precentral.npy    P11_Precentral.npy\n",
    "# P03_Temporal_Inf.npy  P07_Temporal_Inf.npy  P11_Temporal_Inf.npy\n",
    "# P03_Temporal_Mid.npy  P07_Temporal_Mid.npy  P11_Temporal_Mid.npy\n",
    "# P04_Postcentral.npy   P08_Postcentral.npy   P12_Postcentral.npy\n",
    "# P04_Precentral.npy    P08_Precentral.npy    P12_Precentral.npy\n",
    "# P04_Temporal_Inf.npy  P08_Temporal_Inf.npy  P12_Temporal_Inf.npy\n",
    "# P04_Temporal_Mid.npy  P08_Temporal_Mid.npy  P12_Temporal_Mid.npy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# url = 'https://github.com/neurovium/Neuromatch-AJILE12/tree/master/data'\n",
    "# html = requests.get(url).content\n",
    "# soup = BeautifulSoup(html, 'html.parser')\n",
    "# files = [a['href'] for a in soup.select('a.js-navigation-open') if a['href'].endswith('.npy')]\n",
    "\n",
    "# !mkdir -p data\n",
    "# for file in files:\n",
    "#     filename = file.split('/')[-1]\n",
    "#     raw_url = f'https://raw.githubusercontent.com{file.replace(\"/blob\", \"\")}'\n",
    "#     !wget -O data/{filename} {raw_url}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot ECoG electrode locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data characteristics including the number of good and total ECoG electrodes, \n",
    "# # hemisphere implanted, and number of recording days for each participant.\n",
    "from plot_utils import (\n",
    "    load_data_characteristics,\n",
    "    plot_ecog_descript,\n",
    ")\n",
    "\n",
    "dat_chact = load_data_characteristics(fs=fs)\n",
    "n_elecs_good, n_elecs_tot = dat_chact[-2], dat_chact[-1]\n",
    "part_ids = dat_chact[-3]\n",
    "\n",
    "fig = plot_ecog_descript(n_elecs_tot, n_elecs_good, part_ids, nrows=2,fs=fs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project power analyses on different ECOG grids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import modules\n",
    "from plot_utils import plot_ecog_pow\n",
    "\n",
    "# Define variables\n",
    "rois_plt = [\n",
    "    'Precentral',\n",
    "    'Postcentral',\n",
    "    'Temporal_Mid',\n",
    "    'Temporal_Inf'\n",
    "]\n",
    "sbplt_titles = [\n",
    "    'Precentral\\nGyrus',\n",
    "    'Postcentral\\nGyrus',\n",
    "    'Middle Temporal\\nGyrus',\n",
    "    'Inferior Temporal\\nGyrus'\n",
    "]\n",
    "freq_range = [3, 125]\n",
    "lp = 'data/'\n",
    "\n",
    "# Plot power spectra\n",
    "plot_ecog_pow(\n",
    "    lp,\n",
    "    rois_plt,\n",
    "    freq_range,\n",
    "    sbplt_titles,\n",
    "    part_id='P01',\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural activity and movement behavior relationship"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the start and stop times when the behavioral label of interest occurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get coarse labels from NWB file\n",
    "min_len = 100  # (sec) only keep times when the given label appears for longer than this amount of time at once\n",
    "\n",
    "coarse_labels = nwbfile.intervals['epochs'].to_dataframe()\n",
    "coarse_labels = coarse_labels[coarse_labels['labels'].str.contains(behavior_type)]\n",
    "coarse_labels['diff'] = coarse_labels['stop_time'] - coarse_labels['start_time']\n",
    "coarse_labels = coarse_labels[coarse_labels['diff'] > min_len]\n",
    "coarse_labels.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Print the coarse labels as a table\n",
    "coarse_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corresponding ECoG data for each behavioral label chunk and convert to spectral power via the Hilbert transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_order = 4  # order of butterworth filter used to bandpass filter the ECoG data\n",
    "\n",
    "neural_data = nwbfile.acquisition['ElectricalSeries'].data\n",
    "sampling_rate = nwbfile.acquisition['ElectricalSeries'].rate  # (Hz) ECoG sampling rate\n",
    "neural_power = []\n",
    "for i in range(coarse_labels.shape[0]):\n",
    "    # Identify the start/end indices for each continuous chunk of the given behavioral label\n",
    "    start_t = int(coarse_labels.loc[i, 'start_time']*sampling_rate)\n",
    "    end_t = int(coarse_labels.loc[i, 'stop_time']*sampling_rate)\n",
    "\n",
    "    # Load data snippet\n",
    "    neur_data_curr = neural_data[start_t:end_t, ecog_ch_num]\n",
    "\n",
    "    # Bandpass filter\n",
    "    sos = butter(filter_order, neural_freq_range, btype='bandpass', output='sos', fs=sampling_rate)\n",
    "    neur_data_filtered = sosfiltfilt(sos, neur_data_curr)\n",
    "\n",
    "    # Apply Hilbert transform and convert to decibels\n",
    "    neur_pow = np.abs(hilbert(neur_data_filtered))\n",
    "    neur_pow = 10*np.log(neur_pow)\n",
    "\n",
    "    # Take the difference between neighboring timepoints\n",
    "    neural_power.append(np.diff(neur_pow))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the corresponding pose data for each behavioral label chunk and convert to vertical velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = list(nwbfile.processing['behavior'].data_interfaces['Position'].spatial_series.keys())\n",
    "assert keypoint_of_interest in keypoints\n",
    "assert pose_direction in ['vertical', 'horizontal']\n",
    "keypoint_series = nwbfile.processing['behavior'].data_interfaces['Position'].spatial_series[keypoint_of_interest]\n",
    "sampling_rate_keypoint = keypoint_series.rate  # Hz\n",
    "keypoint_velocity = []\n",
    "for i in range(coarse_labels.shape[0]):\n",
    "    start_t = int(coarse_labels.loc[i, 'start_time']*sampling_rate_keypoint)\n",
    "    end_t = int(coarse_labels.loc[i, 'stop_time']*sampling_rate_keypoint)\n",
    "\n",
    "    # Load pose data snippet\n",
    "    pose_data_curr = keypoint_series.data[start_t:end_t, :]\n",
    "    pose_mag_curr = pose_data_curr[:, 1 if pose_direction == 'vertical' else 0]\n",
    "\n",
    "    # Convert to velocity (delta X / delta t)\n",
    "    velocity_curr = np.diff(pose_mag_curr)/(1/sampling_rate_keypoint)\n",
    "    keypoint_velocity.append(velocity_curr)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align and combine neural and pose data into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(neural_power) == len(keypoint_velocity)\n",
    "measures_all = []\n",
    "for i in range(len(neural_power)):\n",
    "    # Neural power for the given chunk\n",
    "    neur_curr = neural_power[i]\n",
    "    l_neur = len(neur_curr)\n",
    "\n",
    "    # Pose velocity for the given chunk\n",
    "    accel_curr = keypoint_velocity[i]\n",
    "    l_accel = len(accel_curr)\n",
    "\n",
    "    # Downsample neural data to match pose data\n",
    "    inds_split = np.array_split(np.arange(l_neur), l_accel)\n",
    "    for j, inds in enumerate(inds_split):\n",
    "        measures_all.append([neur_curr[inds].mean(), accel_curr[j]])\n",
    "\n",
    "# Combine neural/pose data into a dataframe\n",
    "df_measures_all = pd.DataFrame(np.asarray(measures_all), columns=['Neural power (dB)', 'Keypoint velocity (pixels/sec)'])\n",
    "\n",
    "# Remove any NaN's\n",
    "df_measures_all.dropna(inplace=True)\n",
    "\n",
    "# Remove instances with velocity close to 0\n",
    "df_measures_all = df_measures_all[(df_measures_all['Keypoint velocity (pixels/sec)'] > 100) |\\\n",
    "                                  (df_measures_all['Keypoint velocity (pixels/sec)'] < -100)]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Perform robust linear regression to quantify any linear relationships\n",
    "Regression identifies a small, but significant (p<0.05) positive relationship between neural power in the gamma band and right wrist vertical velocity. This result makes sense because moving one's wrist upward takes more effort (fighting against gravity) than moving one's arm downward and thus may require slightly more cortical control."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print correlation between neural power and keypoint velocity\n",
    "We find a small, positive correlation between neural power in the gamma band and right wrist vertical velocity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_measures_all.corr(method='pearson')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data with linear fit\n",
    "Most of the data appears clustered near 0 velocity. The positive relationship between neural power and right wrist vertical velocity is only barely visible. Additional steps that may help better understand the relationship between neural spectral power and wrist velocity include: removing pose data with abnormally high standard deviation due to noisy tracking, subtracting spectral power in nearby periods with minimal movement from ECoG spectral power, and manually reviewing pose trajectories to remove noisy tracking periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(data=df_measures_all, x='Keypoint velocity (pixels/sec)', y='Neural power (dB)', robust=True, ci=None)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
